{{ template "chart.header" . }}

{{ template "chart.deprecationWarning" . }}

{{ template "chart.badgesSection" . }}

{{ template "chart.description" . }}

{{ template "chart.homepageLine" . }}

{{ template "chart.maintainersSection" . }}

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

## Features

- **TensorRT-LLM Build Support**: Automatically builds optimized models using TensorRT-LLM
- **ConfigMap-based Model Configuration**: Manage models via Kubernetes ConfigMaps
- **Flexible Storage**: Support for various storage backends (hostPath, PVC, NFS, etc.)
- **Custom Init Containers**: Extensible with additional init containers
- **Horizontal Pod Autoscaling**: Scale based on CPU, memory, or custom metrics

## Installation

### Install from GHCR

```bash
# Install latest version
helm install triton oci://ghcr.io/metadist/synaplan-charts/triton

# Or install specific version
helm install triton oci://ghcr.io/metadist/synaplan-charts/triton --version {{ template "chart.version" . }}
```

### Install from local chart

```bash
helm install triton ./charts/triton
```

## Configuration

{{ template "chart.valuesSection" . }}

## Model Configuration

Models can be configured via ConfigMaps. Example:

```yaml
models:
  - name: mistral-7b-instruct-v0.3
    files:
      - key: config.pbtxt
        path: config.pbtxt
```

The chart will create ConfigMaps for each model and mount them into the correct paths in the model repository.

## TensorRT-LLM Build

Enable TensorRT-LLM optimization by setting:

```yaml
trtllmBuild:
  enabled: true

tensorRtLlmImage:
  repository: nvcr.io/nvidia/tensorrt-llm/release
  tag: "0.21.0"
```
