# GPU mode configuration for Triton Inference Server

image:
  repository: nvcr.io/nvidia/tritonserver
  pullPolicy: IfNotPresent
  tag: "25.09-pyt-python-py3"

# Enable TensorRT-LLM build for GPU
trtllmBuild:
  enabled: true
  image:
    repository: nvcr.io/nvidia/tensorrt-llm/release
    tag: "0.21.0"

# Resource requests for GPU nodes
resources:
  requests:
    cpu: "2"
    memory: "24Gi"
    nvidia.com/gpu: 1
  limits:
    cpu: "4"
    memory: "24Gi"
    nvidia.com/gpu: 1

# TensorRT-LLM models for GPU inference
models:
  - name: mistral-7b-instruct-v0.3
    files:
      - key: config.pbtxt
        path: config.pbtxt

  - name: mistral-streaming
    files:
      - key: config.pbtxt
        path: config.pbtxt
      - key: model.py
        path: 1/model.py
